input{

	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html
	file {				
		# We use env variables for our paths (experimental):
		# https://www.elastic.co/guide/en/logstash/current/environment-variables.html 
		# path => "${LAPD_HOME}/data/lapd_small_utf8.csv"
                path => "${LAPD_HOME}/data/lapd_crime_and_collision_*_utf8.csv"
		start_position => beginning
		# This is the default charset. See https://www.elastic.co/guide/en/logstash/current/plugins-codecs-plain.html 
		codec => plain { charset => "UTF-8" }
		sincedb_path => "/dev/null"
		}
}

filter {
	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-csv.html
	csv {
		separator => ","
		columns => ["Date Rptd","DR NO","date_occurrence","time_occurrence","area_code","area_name","RD","crime_code","crime_description","Status","Status Desc","address","Cross Street","gps"]
		remove_field => ["Date Rptd","DR NO","RD","Status","Status Desc","Cross Street"]
	}

	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
	mutate {

		#Process gps field (lat,long), remove parenthesis
		# https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-geo-point-type.html#_lat_lon_as_string_6
		gsub => ["gps","\(","","gps","\)","",
			 "date_occurrence"," 12:00:00 AM",""]

		# Process dato
		# Use metadata fields for faster processing: https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html#metadata
		add_field => { "[@metadata][datetime]" => "%{date_occurrence} %{time_occurrence}" }
		remove_field => ["date_occurrence","time_occurrence"]
				
		#Process address: replace several white spaces with one
		strip => ["address"]
		split => {"address" => " "}
		join => { "address" => " "}
	}

	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html
	date {
		# http://joda-time.sourceforge.net/timezones.html
		timezone => "America/Los_Angeles"
		match => [ "[@metadata][datetime]", "MM/dd/yyyy HHmm" ]	
		target => "@timestamp"	
		remove_field => ["datetime"]
	}
	
	# Create an id from "@timestamp" and "crime_code" to uniquely identify our events
	ruby {
		  code => "require 'digest/md5';
		  event['@metadata']['computed_id'] = Digest::MD5.hexdigest((event['@timestamp']).to_s + event['crime_code'])"
		}
}

output{
	# stdout { codec => rubydebug{ metadata => true } }

	if (("_csvparsefailure" not in [tags]) and ("_dateparsefailure" not in [tags]) and ("_rubyexception" not in [tags])){
		# Metadata fields do not show automatically. Use this configuration to show them in the console output
		
		# https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
		elasticsearch {
			hosts => ["127.0.0.1:9200"]
			#index => "lapd-%{+YYYY.MM.dd}" # Use this format to make indices per day
			index => "lapd-%{+YYYY.MM}"  # mÃ¥nelig indices
			document_id => "%{[@metadata][computed_id]}"			
		}		
	} else {
		file {
			path => "${LAPD_HOME}/lapd-parse-failure.csv"
		}
	  }
}
