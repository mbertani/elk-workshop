# 	Change
# 	<full path to elk-workshop>
# 	with the right path in your system
input{
	file {
		# Unfortunately Logstash does not admit relative paths. Windows users: use c:/...
		path => "<full path to elk-workshop>/logstash/pipelines/lapd/data/lapd_small.csv"
		start_position => beginning
		# This is the default charset. See 
		codec => plain { charset => "UTF-8" }
		sincedb_path => "<full path to elk-workshop>/logstash/pipelines/lapd/lapd_small.sincedb"
		}
}

filter {
	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-csv.html
	csv {
		separator => ","
		columns => ["Date Rptd","DR NO","date_occurrence","time_occurrence","area_code","area_name","RD","crime_code","crime_description","Status","Status Desc","address","Cross Street","gps"]
		remove_field => ["Date Rptd","DR NO","RD","Status","Status Desc","Cross Street"]
	}
	
	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
	mutate {
	
		# Process dato
		# Use metadata fields for faster processing: https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html#metadata
		add_field => { "[@metadata][datetime]" => "%{date_occurrence} %{time_occurrence}" }
		remove_field => ["date_occurrence","time_occurrence"]
		
		#Process address: replace several white spaces with one
		strip => ["address"]
		split => {"address" => " "}
		join => { "address" => " "}
		
		#Process gps field (lat,long), remove parenthesis
		# https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-geo-point-type.html#_lat_lon_as_string_6
		gsub => ["gps","\(","","gps","\)",""]
		
	}

	# Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html

	date {
		# http://joda-time.sourceforge.net/timezones.html
		timezone => "America/Los_Angeles"
		match => [ "[@metadata][datetime]", "MM/dd/yyyy HHmm" ]	
		target => "@timestamp"	
		remove_field => ["datetime"]
	}
	
	# Create an id from "@timestamp" and "crime_code" to uniquely identify our events
	ruby {
		  code => "require 'digest/md5';
		  event['@metadata']['computed_id'] = Digest::MD5.hexdigest((event['@timestamp']).to_s + event['crime_code'])"
		}	
}

output{

	if (("_csvparsefailure" not in [tags]) and ("_dateparsefailure" not in [tags]) and ("_rubyexception" not in [tags])){
		# Metadata fields do not show automatically. Use this configuration to show them in the console output
		stdout { codec => rubydebug{ metadata => true } }
		
		# https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
		elasticsearch {
			hosts => ["127.0.0.1:9200"]
			#index => "lapd-%{+YYYY.MM.dd}" # Use this format to make indices per day if your logs are huge
			index => "lapd" 
			document_id => "%{<use the metadata computed_id field here>}"			
		}		
	} else {
		file {
			path => "<full path to elk-workshop>/logstash/pipelines/lapd/lapd-parse-failure.csv"
		}
	  }
}