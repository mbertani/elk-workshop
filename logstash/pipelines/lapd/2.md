# csv filter pluing
https://www.elastic.co/guide/en/logstash/current/plugins-filters-csv.html

You will learn to use the csv filter plugin.

Start with 2.conf and proceed with the excercises under in the given order.
If you need help, you can take a peek at 2.fasit.conf or ask the speakers. 
Try to run logstash at each completed step to make sure you are on the right track. 
Remember to delete the file at sincedb_path between runs, so that logstash process the whole input file again. 

1. Parse lapd_small.csv with the csv filter plugin. Write the following column names in the "column" setting:  
  Date Rptd,DR NO,DATE OCC,TIME OCC,AREA,AREA NAME,RD,Crm Cd,Crm Cd Desc,Status,Status Desc,LOCATION,Cross Street,Location 1

2. Parse lapd_small.csv again, but this time make sure you:
  - rename the following fields:  
    "DATE OCC" => "date_occurrence"  
    "TIME OCC" => "time_occurrence"  
    "AREA" => "area_code"  
    "AREA NAME" => "area_name"  
    "Crm Cd" => "crime_code"  
    "Crm Cd Desc" => "crime_description"  
    "LOCATION" => "address"  
    "Location 1" => "gps"
  - delete the following fields: 
    "Date Rptd","DR NO","RD","Status","Status Desc","Cross Street"
	
# Logic in the output 
When using plugins, you will encounter errors when parsing. Logstash adds "tags" to the events, so that you can check if the parsing succeeded.
You can then redirect these outputs to a file so that you can figure out why they fail and act accordingly.

1. Parse lapd_small.csv again but this time add the following output section (with the right path):
  ```
  output {
    if ("_csvparsefailure" not in [tags]) {
      stdout { codec => rubydebug}
    } else {
      file {
        path => "<full path to elk-workshop>/logstash/pipelines/lapd/lapd-parse-failure.csv"
      }
    }
  }
  ```

2. If the file is not produced, then all events where parsed successfully.
