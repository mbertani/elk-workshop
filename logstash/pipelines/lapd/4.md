# Send to Elasticsearch
Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html

In this excercise you will learn how to send events to elasticsearch. 
The key words are:  hosts, index and document_id. Check them out in the above documenation link.
Furthermore, you will also learn to format the data that comes into elasticsearch by creating a template.

Start with 4.conf and proceed with the excercises under in the given order.
If you need help, you can take a peek at 4.fasit.conf or ask the speakers. 
Try to run logstash at each completed step to make sure you are on the right track. 
Remember to delete the file at sincedb_path between runs, so that logstash process the whole input file again. 

1. Push your events into elasticsearch.

  When sending events to elasticsearch, if you do not specify document_id, a new one will be generated
  each time you send the data. This will create duplicated data if we run logstash several times when
  clearing the sincedb_path. To prevent this, we can create a new field to produce a unique id for 
  each event. Look at the ruby filter. Do you understand what we are doing?

  Look at the elasticsearch output plugin in 4.conf. You need fill the missing code under document_id.

  Open your browser at the Kibana end point: http://127.0.0.1:5601/
  Write "lapd" in the index pattern field and tab out of the field. Time-field name should be filled with "@timestamp" field.
  Click on the "create" button.
  You will see a list with all field names, their type, if they are analyzed and indexed. But everything is of type "string". 
  Even the "gps" field. This is not good since we will not be able to plot our data into a map. 

2. Fixing geo_point fields and choosing what will be analyzed (tokenized)

  1. Go to http://localhost:5601/app/sense
   2. Open ./elk-workshop/logstash/lapd/sense.txt and paste into kibana.
   3. Execute GET lapd/_mapping, we will take a look at the structure of the default mapping from elasticsearch.
  4. Delete your index (yes, this will delete all your data!) by executing DELETE lapd
  5. Put a template to format your data in elasticsearch  by executing PUT _template/lapd_template
    "not_analyzed" 	-> we will use these fields to build aggregations  
    "analyzed" 		-> we will use these fields for search

3. Process the whole dataset for 2014 and 2015. If nothing happens, make sure you touch the files  and run:
```
$ ../../../logstash-2.3.4/bin/logstash  -f 4.fasit.conf --allow-env
```

4. Open http://localhost:9200/_plugin/kopf/ in firefox and look at the creation of the indexes. 
When the cluster stabilizes then we know logstash is done.