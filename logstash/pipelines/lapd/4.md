# Send to Elasticsearch
Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html

In this excercise you will learn how to send events to elasticsearch. 
The key words are:  hosts, index and document_id. Check them out in the above documenation link.
Furthermore, you will also learn to format the data that comes into elasticsearch by creating a template.

Start with 4.conf and proceed with the excercises under in the given order.
If you need help, you can take a peek at 4.fasit.conf or ask the speakers. 
Try to run logstash at each completed step to make sure you are on the right track. 
Remember to delete the file at sincedb_path between runs, so that logstash process the whole input file again. 

1. Push your events into elasticsearch.

When sending events to elasticsearch, if you do not specify document_id, a new one will be generated
each time you send the data. This will create duplicated data if we run logstash several times when
clearing the sincedb_path. To prevent this, we can create a new field to produce a unique id for 
each event. Look at the ruby filter. Do you understand what we are doing?

Look at the elasticsearch output plugin in 4.conf. You need fill the missing code under document_id.

Open your browser at the Kibana end point: http://127.0.0.1:5601/
Write "lapd" in the index pattern field and tab out of the field. Time-field name should be filled with "@timestamp" field.
Click on the "create" button.
You will see a list with all field names, their type, if they are analyzed and indexed. But everything is of type "string". 
Even the "gps" field. This is not good since we will not be able to plot our data into a map. 

2. Fixing geo_point fields and choosing what will be analyzed (tokenized)

2.a. Get your mapping from http://localhost:9200/lapd?pretty

{
  "lapd" : {
    "aliases" : { },
    "mappings" : {
      "logs" : {
        "properties" : {
          "@timestamp" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "@version" : {
            "type" : "string"
          },
          "address" : {
            "type" : "string"
          },
          "area_code" : {
            "type" : "string"
          },
          "area_name" : {
            "type" : "string"
          },
          "crime_code" : {
            "type" : "string"
          },
          "crime_description" : {
            "type" : "string"
          },
          "gps" : {
            "type" : "double"
          },
          "host" : {
            "type" : "string"
          },
          "message" : {
            "type" : "string"
          },
          "path" : {
            "type" : "string"
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1453240549960",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "tqlp1zrpRuCW20bU33kZBg",
        "version" : {
          "created" : "2010199"
        }
      }
    },
    "warmers" : { }
  }
}

2.b. Delete your index (yes, this will delete all your data!)
> curl -Method DELETE -Uri http://127.0.0.1:9200/lapd
$ curl -XDELETE 'http://127.0.0.1:9200/lapd'

2.c. Put a template to format your data in elasticsearch
"not_analyzed" 	-> we will use these fields to build aggregations
"analized" 		-> we will use these fields for search

> curl -Method DELETE -Uri http://127.0.0.1:9200/_template/lapd_template
$ curl -XDELETE 'http://127.0.0.1:9200/_template/lapd_template'

> curl -Method Put -Uri http://127.0.0.1:9200/_template/lapd_template -Body '
{
"template" : "lapd",
    "mappings" : {
      "logs" : {
        "properties" : {
          "@timestamp" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "@version" : {
            "type" : "string"
          },
          "address" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "area_code" : {
            "type" : "string", "index": "not_analyzed"
          },
          "area_name" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "crime_code" : {
            "type" : "string", "index": "not_analyzed"
          },
          "crime_description" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "gps" : {
            "type": "geo_point", "lat_lon" : true, "geohash" : true, "index": "not_analyzed", "fielddata" : {"format" : "compressed","precision" : "1cm"}
          },
          "host" : {
            "type" : "string", "index": "not_analyzed"
          },
          "message" : {
            "type" : "string"
          },
          "path" : {
            "type" : "string", "index": "not_analyzed"
          }
	  }
    }
  }
}'

$ curl -XPUT 'http://127.0.0.1:9200/_template/lapd_template' -d '{
"template" : "lapd",
    "mappings" : {
      "logs" : {
        "properties" : {
          "@timestamp" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "@version" : {
            "type" : "string"
          },
          "address" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "area_code" : {
            "type" : "string", "index": "not_analyzed"
          },
          "area_name" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "crime_code" : {
            "type" : "string", "index": "not_analyzed"
          },
          "crime_description" : {
            "type" : "string", "fields": {"raw": {"type":  "string","index": "not_analyzed"}}
          },
          "gps" : {
            "type": "geo_point", "lat_lon" : true, "geohash" : true, "index": "not_analyzed"
          },
          "host" : {
            "type" : "string", "index": "not_analyzed"
          },
          "message" : {
            "type" : "string"
          },
          "path" : {
            "type" : "string", "index": "not_analyzed"
          }
        }
      }
    }
}'

2.d. Look at your template at http://127.0.0.1:9200/_template/lapd_template?pretty

2.e. Run Logstash again and check in kibana. 

3. Process the whole dataset, but add a new parameter to the command: "-w 5". This tells logstash to start 5 workers
who will help in processing the data. Try with a higher number if your machine allows you to.

> ..\..\..\logstash-2.1.1\bin\logstash agent -f 4.conf -w 5
$ ../../../logstash-2.1.1/bin/logstash agent -f 4.conf -w 5 

Now you are ready to process the whole dataset into elasticsearch, so that we can start visualizing data in kibana.

In the file input plugin, change the path setting to point to lapd_crime_and_collision_2014.csv. 
In the output section, comment the stdout output plugin, for at logstash process the data faster.
Run the logstash job and check in Kibana when the import is done.

4. Creating visualizations in Kibana
5. Making a dashboard
