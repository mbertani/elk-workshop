# Clean and format your data
You will learn to parse dates and to clean your data.

"date_occurrence" and "time_occurrence" track our date and time values. 
We want to create a field called "datetime" where we join the date and time information, separated by a white space. 
Every event in logstash has a "@timestamp" value. This corresponds to the date and time when the event is processed. 
For our purposes, this is not important, and we want to overwrite "@timestamp" with "datetime". Then we delete "datetime" as it is no longer in use.

Furthermore, we want to eliminate the extra white spaces from address field.

Start with 3.conf and proceed in the following order, by uncommenting the corresponding code.
If you need help, you can take a peek at 3.fasit.conf or ask the speakers. 
Try to run logstash at each completed step to make sure you are on the right track. 
Remember to delete the file at sincedb_path between runs, so that logstash process the whole input file again. 

1. Use the mutate filter to join date_occurence and time_occurrence into a new field "datetime".
Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
Look at the add_field setting. You need to find the right way to address "date_occurrence" and "time_occurrence" fields.

2. Remove date_occurence and time_occurrence. Look at remove_field setting from the mutate filter.

3. Overwrite "@timestamp" with "datetime". For this, you need to use the date filter
Documentation: https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html

  Here you will find the right timezone http://joda-time.sourceforge.net/timezones.html, 
  write the date format in the "match" setting (by looking at sample data), 
  put the data into the @timestamp field and remove the datetime field.

  Take a look at the output filter. We have added a _dateparsefailure condition, to catch the events which fail when parsing. Look at the output file, and see what events appear there.

  3*. For the curious ones, there is a way in logstash to process the data faster, which is by using @metadata.
  Look at https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html#metadata
  Check 3.fasit for how we join and parse "date_occurrence" and "time_occurrence" with @metadata. 
  These fields are processed in another way, and you don't need to delete them, 
  since metadata fields are not oversent to elasticsearch.

4. Clean the address field.
  In the mutate filter documentation https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
  look at strip,split and join. Can you combine these in order to remove all double ocurrences of white spaces?

5. Clean the gps field. Remove parenthesis.
  There are several ways to send gps coordinates to elasticsearch. In our case, the easiest is to use the string version:  
  https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-geo-point-type.html#_lat_lon_as_string_6

  Our data is already in the right order, but we have to remove parenthesis from the gps field. 
  Use gsub to achieve this.
