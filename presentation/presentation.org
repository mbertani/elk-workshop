#+OPTIONS: toc:nil email:nil H:4 num:nil ^:nil
#+TITLE: Hands-on ELK Workshop
#+AUTHOR: Marco Bertani-Økland and Sigmund Hansen
#+EMAIL: mab@computas.com sha@computas.com
#+REVEAL_THEME: night

* Agenda for today
|---------------|--------------------------------------|
| *17:00-17:30* | Intro + food (we speak, you eat )    |
| *17:30-18:00* | Configuration (Install ELK + repo)   |
| *18:00-18:40* | 1st pipeline logstash + Elasticsearch|
| *18:40-18:50* | pause                                |
| *18:50-19:20* | 1st pipeline visualization in Kibana |
| *19:20-19:30* | Pause                                |  
| *19:30-20:10* | 2nd pipeline logstash + Elasticsearch|
| *20:10-20:20* | Pause                                |
| *20:20-21:00* | 2nd pipeline visualization in Kibana |
|-------------+--------------------------------------|


* What is the *ELK* platform?


ELK consist of three open source projects — Elasticsearch, Logstash, and Kibana 
— designed to take data from any source and search, analyze, and visualize it in real time. 
The philosophy behind these tools is that getting immediate, actionable insight from data matters.
#+ATTR_REVEAL: :frag (appear)
- *Elasticsearch* for deep search and data analytics. 
- *Logstash* for centralized logging management: shipping and forwarding logs, log enrichment, and parsing.
- *Kibana* for powerful and beautiful data visualizations. 

**  What can we use ELK for?
 
- Issue debugging
- Performance analysis
- Security analysis
- Predictive analysis
- Internet of things (IoT) and logging

** Typical problems with your logs

- Non-consistent log format
- Decentralized logs
- Expert knowledge requirement


* Setup
- Clone https://github.com/mbertani/elk-workshop
- Extract at the root of the repository:
  - Elasticsearch
  - Logstash
  - Kibana
- Edit elasticsearch-2.1.1/config/elasticsearch.yml
  - cluster.name: ${HOSTNAME}

** Shell

- Start a shell in <elk-workshop>/elasticsearch-2.1.1/bin
  - Run elasticsearch
  - Open your browser at http://127.0.0.1:9200
- Start a shell in <elk-workshop>/kibana-4.3.1-xxx/bin
  - Run kibana
  - Open your browser at http://127.0.0.1:5601
- Start a shell in <elk-workshop>/logstash/pipelines/setup
  - This will be used for logstash 


** Verify Logstash

- Follow instructions at setup.txt

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f verify.conf --configtest
#+END_SRC

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f verify.conf --configtest
#+END_SRC

* Pipelines

- LAPD Crime Reports with Marco
- HTTP Access Logs with Sigmund

* LAPD Crime Reports

- Navigate to *./logstash/pipelines/lapd*
- Familiarize yourself with the data *./logstash/pipelines/lapd/data/lapd_small.csv*
- We will focus on the following headers:

|-------------+------------------------|
| DATE OCC    | Date of occurrence     |
| TIME OCC    | Time of occurrence     |
| Crm Cd      | Crime Code             |
| Crm Cd Desc | Crime Code Description |
| Status      |                        |
| Statue Desc |                        |
| LOCATION    | Street address         |
| location 1  | GPS coordinates        |
|-------------+------------------------|
** 1st step: Read the data

| *What:* | Learn how to use the file input plugin |
| *How:*  | Open 1.txt and roll up your sleeves |
| *When:* | Now. You have 3 minutes! |

*Ærg help!* https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html

** 2nd step: Give structure to the data

| *What:* | Familiarize yourself with the csv filter plugin |
| *How:*  | Open 2.txt and read.|
| *When:* | Now. You have 5 minutes! |

*Ærg help!* 

https://www.elastic.co/guide/en/logstash/current/plugins-filters-csv.html

** 3rd step: Clean and format the data

| *What:*  | Familiarize yourself with mutate and date filter plugins |
| *How:* | Open 3.txt |
| *When:* | Now. You have 5 minutes! |

*Ærg help!*  

https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html

https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html 

** 4th step: Export data to elasticsearch
| *What:*  | Familiarize yourself with elasticsearch output plugin    |
| *How:* | Open 4.txt |
| *When:* | Now. You have 5 minutes! |

*Ærg help!*  

https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html

** Kibana visualization

#+ATTR_REVEAL: :frag (appear)
- Settings tab
  - Get lapd index
- Discover tab
  - Play with the time filter
  - See the structure of the data
- Visualize tab
  - Generate Pie charts
  - Histogram bars
  - Line charts for trends
  - Metrics
  - Filter aggregations
- Dashboard tab
  - Construct a dashboard
  - How to import / export the dashboard


* HTTP Access Logs

Access logs generated by a script based on: \\
https://gist.github.com/fetep/2037301

Logs, exercises and configuration files can be found in *logstash/pipelines/httpd*

** Grok

- Regular expression text parser
- Pre-defined patterns
  - See: https://github.com/logstash-plugins/logstash-patterns-core/
- Named matches become fields

*** Getting started

- Have a look at *data/access.mini.log*
- Adapt the paths in *1.conf*
- Run logstash and take note of the *test* field:

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f 1.conf
#+END_SRC

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f 1.conf
#+END_SRC

**** Match Option

+ Take note of the pattern used: "%{DATA:test} "
+ *DATA* is a pre-defined pattern equivalent to ".*?"
+ *:test* tells grok to bind the match to the field *test*
+ "%{DATA:test} " is equivalent to "(?<test>.*?) "

*** Grok constructor

- Regular expressions can be a hassle
- Lots of pre-defined patterns (around 120): \\
  https://github.com/logstash-plugins/logstash-patterns-core/
- http://grokconstructor.appspot.com/ \\
  to the rescue

**** Incremental Construction

- Select incremental construction
- Copy a few lines from access.mini.log into the text area and press Go
- Notice that the first pattern in the list matches everything: \\
  *COMBINEDAPACHELOG*
  - In the final results, we will use this pattern. \\
    For now, spend a few minutes getting familiar with the constructor.

**** Incremental Construction cont.

- The Apache log format documentation: \\
  https://httpd.apache.org/docs/1.3/logs.html#common
- Try to build a pattern that will capture the following fields:
  - Client IP/host name
  - Date and time
  - HTTP method
  - Path part of requested URL
  - HTTP status code
- Feel free to handle more parts
- Remember to add field names to the pattern
- Test your patterns

** Geo IP

- Adds GPS coordinates based on IP addresses.
- A database mapping IP addresses to cities is included in logstash.
- Updated databases can be downloaded from \\
  http://dev.maxmind.com/geoip/legacy/geolite/

*** Basic Geo IP Configuration

- Use *2.conf*, or add a geoip filter after your grok filter
- First set the source field to the client IP/host name field
- You can find the field by examining the COMMONAPACHELOG pattern \\
  or by running the configuration before adding the geoip filter
- Try running logstash with the configuration

*** Fields

- The geoip has added a lot of fields
- The most important one is *[geoip][location]* (coordinates)
- All these fields take up additional storage space
- Add a *fields* option to the geoip filter and specify a string array of fields you want to keep
- Re-run logstash with the updated configuration

** Timestamp

- Use *3.conf*, for this and the next exercise
- Format specification can be found at: \\
  http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html
- Add a date filter similar to the one used in the LAPD exercise
- You don't need to specify the time zone, \\
  because the Apache date format contains it

** Checksum

- Add a checksum with the checksum filter: \\
  https://www.elastic.co/guide/en/logstash/current/plugins-filters-checksum.html
- Set the algorithm to sha256 (default) or md5
- Set the keys to use the *message* field only
- You cannot specify the output field, so we move it with a mutate
  - Add a *[@metadata][computed_id]* field with the value of the *logstash_checksum* field
  - Remove the *logstash_checksum* field

** Output to Elasticsearch

- Add output to Elasticsearch
- Set the name of the index

*** Import Full Access Log

- Unzip the *data/access.zip* archive
- Run logstash with the final configuration

* Wrap-up

** Useful links

- Follow the blog https://www.elastic.co/blog
- Some books
 - https://www.packtpub.com/big-data-and-business-intelligence/elasticsearch-cookbook
 - https://www.packtpub.com/big-data-and-business-intelligence/learning-elk-stack 

** Unit/Integration Tests

- Testing Logstash configurations can be difficult
- It is possible to write unit tests in Ruby:
- http://stackoverflow.com/questions/18823917/how-to-implement-the-unit-or-integration-tests-for-logstash-configuration
** Time-based Indices

- You can add date fields to the index name
  - Slight increase in storage requirements
  - Allows deleting partial data, which saves storage
  - Increased performance?
- You may want indices to be:
  - Daily: "-%{+YYYY.MM.dd}"
  - Weekly "-%{+xxxx.ww}"
  - Monthly "-%{+YYYY.MM}"
- Defaults to daily: "logstash-%{+YYYY.MM.dd}"
